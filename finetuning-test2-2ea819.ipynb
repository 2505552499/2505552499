{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d3c5f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-05T07:35:10.241976Z",
     "iopub.status.busy": "2024-08-05T07:35:10.240978Z",
     "iopub.status.idle": "2024-08-05T07:35:10.247863Z",
     "shell.execute_reply": "2024-08-05T07:35:10.245857Z",
     "shell.execute_reply.started": "2024-08-05T07:35:10.241927Z"
    },
    "papermill": {
     "duration": 0.006145,
     "end_time": "2025-05-10T04:17:44.757638",
     "exception": false,
     "start_time": "2025-05-10T04:17:44.751493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credits goes to @tascj0 [is just copy of his team best solution](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e5e49",
   "metadata": {
    "papermill": {
     "duration": 0.005155,
     "end_time": "2025-05-10T04:17:44.768393",
     "exception": false,
     "start_time": "2025-05-10T04:17:44.763238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753e9af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:17:44.780430Z",
     "iopub.status.busy": "2025-05-10T04:17:44.779701Z",
     "iopub.status.idle": "2025-05-10T04:18:30.346579Z",
     "shell.execute_reply": "2025-05-10T04:18:30.345496Z"
    },
    "papermill": {
     "duration": 45.580211,
     "end_time": "2025-05-10T04:18:30.353660",
     "exception": false,
     "start_time": "2025-05-10T04:17:44.773449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\r\n",
      "Installing collected packages: triton\r\n",
      "Successfully installed triton-2.2.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf8cbe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:18:30.366702Z",
     "iopub.status.busy": "2025-05-10T04:18:30.366382Z",
     "iopub.status.idle": "2025-05-10T04:19:17.854621Z",
     "shell.execute_reply": "2025-05-10T04:19:17.853467Z"
    },
    "papermill": {
     "duration": 47.503315,
     "end_time": "2025-05-10T04:19:17.862381",
     "exception": false,
     "start_time": "2025-05-10T04:18:30.359066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\r\n",
      "Installing collected packages: xformers\r\n",
      "Successfully installed xformers-0.0.24+042abc8.d20240802\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2521c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:17.876160Z",
     "iopub.status.busy": "2025-05-10T04:19:17.875888Z",
     "iopub.status.idle": "2025-05-10T04:19:17.879932Z",
     "shell.execute_reply": "2025-05-10T04:19:17.879240Z"
    },
    "papermill": {
     "duration": 0.013423,
     "end_time": "2025-05-10T04:19:17.881616",
     "exception": false,
     "start_time": "2025-05-10T04:19:17.868193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers peft accelerate bitsandbytes \\-U --no-index --find-links /kaggle/input/lmsys-wheel-files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3378365d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:17.893679Z",
     "iopub.status.busy": "2025-05-10T04:19:17.893475Z",
     "iopub.status.idle": "2025-05-10T04:19:18.990126Z",
     "shell.execute_reply": "2025-05-10T04:19:18.989155Z"
    },
    "papermill": {
     "duration": 1.105093,
     "end_time": "2025-05-10T04:19:18.992245",
     "exception": false,
     "start_time": "2025-05-10T04:19:17.887152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/lmsys-modules-0805 human_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2b1c58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:19.005291Z",
     "iopub.status.busy": "2025-05-10T04:19:19.005021Z",
     "iopub.status.idle": "2025-05-10T04:19:19.376366Z",
     "shell.execute_reply": "2025-05-10T04:19:19.375471Z"
    },
    "papermill": {
     "duration": 0.379794,
     "end_time": "2025-05-10T04:19:19.378092",
     "exception": false,
     "start_time": "2025-05-10T04:19:18.998298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/sympy-1.12.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/urllib3-2.2.2-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/tqdm-4.66.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/idna-3.7-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/jinja2-3.1.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/mpmath-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/networkx-3.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/filelock-3.15.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/certifi-2024.7.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/packaging-24.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/requests-2.32.3-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/typing_extensions-4.12.2-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/huggingface_hub-0.23.4-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/fsspec-2024.6.1-py3-none-any.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-wheel-files/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model.safetensors.index.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00003-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/config.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00001-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/tokenizer.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/tokenizer_config.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00004-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/special_tokens_map.json\n",
      "/kaggle/input/lmsys-checkpoints-3-0805/model-00002-of-00004.safetensors\n",
      "/kaggle/input/lmsys-modules-0805/utils.py\n",
      "/kaggle/input/lmsys-modules-0805/models/modeling_gemma2.py\n",
      "/kaggle/input/lmsys-modules-0805/models/modeling_llama.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/triton_utils.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/rms_norm.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/flash_attention_nopad.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/fused_rotary_emb.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/gelu_and_mul.py\n",
      "/kaggle/input/lmsys-modules-0805/models/ops/silu_and_mul.py\n",
      "/kaggle/input/lmsys-modules-0805/data/collators.py\n",
      "/kaggle/input/lmsys-modules-0805/data/dataset.py\n",
      "/kaggle/input/lmsys-modules-0805/data/processors.py\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-4800/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-2000/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-3600/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-2200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-5600/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-5400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-1200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-1400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-3000/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-4400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/.ipynb_checkpoints/dataset-metadata-checkpoint.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-600/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-5748/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-5000/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-4600/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-2400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-4200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-3400/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-1800/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-3800/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-800/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-800/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-800/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-800/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-800/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-800/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-800/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-800/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-800/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-800/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-800/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-800/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-1600/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-4000/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-3200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-1000/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-5200/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-2800/tokenizer.model\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/adapter_model.safetensors\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/trainer_state.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/training_args.bin\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/adapter_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/README.md\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/tokenizer.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/tokenizer_config.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/scheduler.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/special_tokens_map.json\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/optimizer.pt\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/rng_state.pth\n",
      "/kaggle/input/73zap2gx/checkpoint-2600/tokenizer.model\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model.safetensors.index.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00003-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/config.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00001-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer_config.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00004-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/special_tokens_map.json\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/model-00002-of-00004.safetensors\n",
      "/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.model\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model.safetensors.index.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer_config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/special_tokens_map.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8a089",
   "metadata": {
    "papermill": {
     "duration": 0.006133,
     "end_time": "2025-05-10T04:19:19.391096",
     "exception": false,
     "start_time": "2025-05-10T04:19:19.384963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656f4e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:19.404710Z",
     "iopub.status.busy": "2025-05-10T04:19:19.404469Z",
     "iopub.status.idle": "2025-05-10T04:19:19.410743Z",
     "shell.execute_reply": "2025-05-10T04:19:19.409929Z"
    },
    "papermill": {
     "duration": 0.015443,
     "end_time": "2025-05-10T04:19:19.412694",
     "exception": false,
     "start_time": "2025-05-10T04:19:19.397251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prepare_test_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_test_file.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "df[\"winner_model_a\"] = 1\n",
    "df[\"winner_model_b\"] = 0\n",
    "df[\"winner_tie\"] = 0\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "df[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n",
    "df.to_parquet(\"test_swap.parquet\", index=False)\n",
    "\n",
    "###/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取主要的訓練資料集和 reward 分數資訊\n",
    "full_data = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")  # 包含 id, prompt, response_a, response_b 等欄位\n",
    "rewards_data = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")  # 包含 id, fold\n",
    "\n",
    "# 合併 reward 分數到主要資料集中\n",
    "merged_data = full_data.merge(rewards_data, on='id', how='inner')\n",
    "merged_data.rename(columns={'fold': 'reward'}, inplace=True)  # 將 'fold' 欄位重命名為 'reward'\n",
    "\n",
    "# 檢查合併後的資料\n",
    "print(merged_data.head())\n",
    "\n",
    "# 儲存合併後的資料\n",
    "merged_data.to_parquet(\"train_with_rewards.parquet\", index=False)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "# %%writefile prepare_test_file.py\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取原始測試資料和 reward 資料\n",
    "test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "folds_df = pd.read_csv(\"/kaggle/input/llm-finetuning-dataset/train_folds_lmsys.csv\")\n",
    "\n",
    "# 合併 reward 資料到測試資料中\n",
    "merged_df = test_df.merge(folds_df, on=\"id\", how=\"inner\")\n",
    "\n",
    "# 假設 fold 欄位即為 reward 分數，設置 winner labels\n",
    "merged_df[\"winner_model_a\"] = (merged_df[\"fold\"] == 0).astype(int)\n",
    "merged_df[\"winner_model_b\"] = (merged_df[\"fold\"] == 1).astype(int)\n",
    "merged_df[\"winner_tie\"] = (merged_df[\"fold\"] == 2).astype(int)\n",
    "\n",
    "# 保存資料\n",
    "merged_df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "# 創建 response_a 和 response_b 調換版本的資料集\n",
    "merged_df[\"response_a\"], merged_df[\"response_b\"] = merged_df[\"response_b\"], merged_df[\"response_a\"]\n",
    "merged_df.to_parquet(\"test_swap.parquet\", index=False)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da1577a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:19.427366Z",
     "iopub.status.busy": "2025-05-10T04:19:19.426657Z",
     "iopub.status.idle": "2025-05-10T04:19:21.512709Z",
     "shell.execute_reply": "2025-05-10T04:19:21.511789Z"
    },
    "papermill": {
     "duration": 2.095215,
     "end_time": "2025-05-10T04:19:21.514749",
     "exception": false,
     "start_time": "2025-05-10T04:19:19.419534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python prepare_test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d1b0f",
   "metadata": {
    "papermill": {
     "duration": 0.006242,
     "end_time": "2025-05-10T04:19:21.527796",
     "exception": false,
     "start_time": "2025-05-10T04:19:21.521554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: gemma2-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794021cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:21.542231Z",
     "iopub.status.busy": "2025-05-10T04:19:21.541498Z",
     "iopub.status.idle": "2025-05-10T04:19:21.548169Z",
     "shell.execute_reply": "2025-05-10T04:19:21.547324Z"
    },
    "papermill": {
     "duration": 0.015645,
     "end_time": "2025-05-10T04:19:21.549703",
     "exception": false,
     "start_time": "2025-05-10T04:19:21.534058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m0.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "\n",
    "model_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\n",
    "csv_path = \"test.parquet\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=False,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model for pipelined inference\n",
    "num_hidden_layers = 42\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# inv_freq clones for each device\n",
    "config = model.config\n",
    "dim = config.head_dim\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.device)\n",
    "# for name, b in model.model.named_buffers():\n",
    "#     print(name, b.device)\n",
    "\n",
    "# pipeline parallelism with two GPUs\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# last micro-batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "np.save('prob_m0.npy', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d8226b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:19:21.564039Z",
     "iopub.status.busy": "2025-05-10T04:19:21.563441Z",
     "iopub.status.idle": "2025-05-10T04:23:50.417786Z",
     "shell.execute_reply": "2025-05-10T04:23:50.416721Z"
    },
    "papermill": {
     "duration": 268.863669,
     "end_time": "2025-05-10T04:23:50.419755",
     "exception": false,
     "start_time": "2025-05-10T04:19:21.556086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [04:01<00:00, 60.33s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-05-10 04:23:33.563201: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-05-10 04:23:33.563345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-05-10 04:23:33.688707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.79s/it]\r\n",
      "{'log_loss': 3.094615495202658}\r\n"
     ]
    }
   ],
   "source": [
    "!python predict_m0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a6e71",
   "metadata": {
    "papermill": {
     "duration": 0.006591,
     "end_time": "2025-05-10T04:23:50.433502",
     "exception": false,
     "start_time": "2025-05-10T04:23:50.426911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23f6341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:23:50.450116Z",
     "iopub.status.busy": "2025-05-10T04:23:50.449786Z",
     "iopub.status.idle": "2025-05-10T04:23:50.457651Z",
     "shell.execute_reply": "2025-05-10T04:23:50.456830Z"
    },
    "papermill": {
     "duration": 0.018879,
     "end_time": "2025-05-10T04:23:50.459567",
     "exception": false,
     "start_time": "2025-05-10T04:23:50.440688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m3.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "\n",
    "model_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\n",
    "csv_path = \"test_swap.parquet\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.deprecation_warnings[\n",
    "    \"sequence-length-is-longer-than-the-specified-maximum\"\n",
    "] = True\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=True,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model for pipelined inference\n",
    "num_hidden_layers = 32\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# inv_freq clones for each device\n",
    "config = model.config\n",
    "dim = config.hidden_size // config.num_attention_heads\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.device)\n",
    "# for name, b in model.model.named_buffers():\n",
    "#     print(name, b.device)\n",
    "\n",
    "# pipeline parallelism with two GPUs\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# last micro-batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "np.save('prob_m3.npy', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e224b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:23:50.476763Z",
     "iopub.status.busy": "2025-05-10T04:23:50.476492Z",
     "iopub.status.idle": "2025-05-10T04:27:09.669323Z",
     "shell.execute_reply": "2025-05-10T04:27:09.668197Z"
    },
    "papermill": {
     "duration": 199.203584,
     "end_time": "2025-05-10T04:27:09.671534",
     "exception": false,
     "start_time": "2025-05-10T04:23:50.467950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [03:05<00:00, 46.48s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-05-10 04:27:01.605635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-05-10 04:27:01.605706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-05-10 04:27:01.607469: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.66s/it]\r\n",
      "{'log_loss': 0.7582519183970864}\r\n"
     ]
    }
   ],
   "source": [
    "!python predict_m3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4597ba2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:27:09.688396Z",
     "iopub.status.busy": "2025-05-10T04:27:09.688118Z",
     "iopub.status.idle": "2025-05-10T04:27:09.694039Z",
     "shell.execute_reply": "2025-05-10T04:27:09.693321Z"
    },
    "papermill": {
     "duration": 0.016071,
     "end_time": "2025-05-10T04:27:09.695625",
     "exception": false,
     "start_time": "2025-05-10T04:27:09.679554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98613375 0.0021362  0.01173002]\n",
      " [0.13743691 0.5760938  0.28646934]\n",
      " [0.7586595  0.09096359 0.1503769 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "prob = np.load('prob_m3.npy')\n",
    "\n",
    "print(prob[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e69337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:27:09.711365Z",
     "iopub.status.busy": "2025-05-10T04:27:09.711152Z",
     "iopub.status.idle": "2025-05-10T04:27:21.742694Z",
     "shell.execute_reply": "2025-05-10T04:27:21.741801Z"
    },
    "papermill": {
     "duration": 12.041521,
     "end_time": "2025-05-10T04:27:21.744657",
     "exception": false,
     "start_time": "2025-05-10T04:27:09.703136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lmsys-wheel-files\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Installing collected packages: bitsandbytes, peft\r\n",
      "Successfully installed bitsandbytes-0.43.1 peft-0.11.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae941e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:27:21.762362Z",
     "iopub.status.busy": "2025-05-10T04:27:21.762097Z",
     "iopub.status.idle": "2025-05-10T04:27:28.540837Z",
     "shell.execute_reply": "2025-05-10T04:27:28.539993Z"
    },
    "papermill": {
     "duration": 6.789792,
     "end_time": "2025-05-10T04:27:28.542807",
     "exception": false,
     "start_time": "2025-05-10T04:27:21.753015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 04:27:25.366038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-10 04:27:25.366098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-10 04:27:25.367788: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%writefile predict_lora.py\n",
    "\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "\n",
    "assert torch.cuda.device_count() == 2\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n",
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c717134d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:27:28.561099Z",
     "iopub.status.busy": "2025-05-10T04:27:28.560839Z",
     "iopub.status.idle": "2025-05-10T04:27:29.520622Z",
     "shell.execute_reply": "2025-05-10T04:27:29.519658Z"
    },
    "papermill": {
     "duration": 0.970522,
     "end_time": "2025-05-10T04:27:29.522459",
     "exception": false,
     "start_time": "2025-05-10T04:27:28.551937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 675 ms, sys: 174 ms, total: 849 ms\n",
      "Wall time: 954 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b36c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:27:29.540337Z",
     "iopub.status.busy": "2025-05-10T04:27:29.540092Z",
     "iopub.status.idle": "2025-05-10T04:28:54.150422Z",
     "shell.execute_reply": "2025-05-10T04:28:54.149703Z"
    },
    "papermill": {
     "duration": 84.621143,
     "end_time": "2025-05-10T04:28:54.152357",
     "exception": false,
     "start_time": "2025-05-10T04:27:29.531214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd903f4d8614d94bfcd791f5f251073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d2e975197e4a78b02f36c81cdbe587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4665f396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:54.172117Z",
     "iopub.status.busy": "2025-05-10T04:28:54.171847Z",
     "iopub.status.idle": "2025-05-10T04:28:54.178550Z",
     "shell.execute_reply": "2025-05-10T04:28:54.177816Z"
    },
    "papermill": {
     "duration": 0.017787,
     "end_time": "2025-05-10T04:28:54.180080",
     "exception": false,
     "start_time": "2025-05-10T04:28:54.162293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile predict_qlora.py\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e58b035a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:54.198005Z",
     "iopub.status.busy": "2025-05-10T04:28:54.197362Z",
     "iopub.status.idle": "2025-05-10T04:28:54.200620Z",
     "shell.execute_reply": "2025-05-10T04:28:54.200007Z"
    },
    "papermill": {
     "duration": 0.013775,
     "end_time": "2025-05-10T04:28:54.202100",
     "exception": false,
     "start_time": "2025-05-10T04:28:54.188325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python predict_qlora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9887d839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:54.220382Z",
     "iopub.status.busy": "2025-05-10T04:28:54.219946Z",
     "iopub.status.idle": "2025-05-10T04:28:59.376658Z",
     "shell.execute_reply": "2025-05-10T04:28:59.375847Z"
    },
    "papermill": {
     "duration": 5.16739,
     "end_time": "2025-05-10T04:28:59.378297",
     "exception": false,
     "start_time": "2025-05-10T04:28:54.210907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>length</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[2, 235322, 39038, 78880, 2250, 577, 23414, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1699</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.644424</td>\n",
       "      <td>0.209521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[2, 235322, 39038, 78880, 590, 791, 2149, 7263...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.962159</td>\n",
       "      <td>0.031446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[2, 235322, 39038, 78880, 1646, 708, 476, 9876...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>452</td>\n",
       "      <td>0.340726</td>\n",
       "      <td>0.243892</td>\n",
       "      <td>0.415382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          input_ids  \\\n",
       "2  1233961  [2, 235322, 39038, 78880, 2250, 577, 23414, 57...   \n",
       "0   136060  [2, 235322, 39038, 78880, 590, 791, 2149, 7263...   \n",
       "1   211333  [2, 235322, 39038, 78880, 1646, 708, 476, 9876...   \n",
       "\n",
       "                                      attention_mask  length  winner_model_a  \\\n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    1699        0.146055   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      63        0.006395   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     452        0.340726   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "2        0.644424    0.209521  \n",
       "0        0.962159    0.031446  \n",
       "1        0.243892    0.415382  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 5.13910436630249\n",
      "elapsed time: 0.0002143383026123047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.644424</td>\n",
       "      <td>0.209521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.962159</td>\n",
       "      <td>0.031446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.340726</td>\n",
       "      <td>0.243892</td>\n",
       "      <td>0.415382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "2  1233961        0.146055        0.644424    0.209521\n",
       "0   136060        0.006395        0.962159    0.031446\n",
       "1   211333        0.340726        0.243892    0.415382"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "display(result_df)\n",
    "\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a89995a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:59.397045Z",
     "iopub.status.busy": "2025-05-10T04:28:59.396747Z",
     "iopub.status.idle": "2025-05-10T04:28:59.403937Z",
     "shell.execute_reply": "2025-05-10T04:28:59.402981Z"
    },
    "papermill": {
     "duration": 0.018086,
     "end_time": "2025-05-10T04:28:59.405586",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.387500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14605489 0.64442444 0.20952067]\n",
      " [0.00639494 0.96215874 0.03144629]\n",
      " [0.34072578 0.24389245 0.41538179]]\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(submission_df)\n",
    "\n",
    "prob_qlora = submission_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].to_numpy()\n",
    "\n",
    "print(prob_qlora)\n",
    "np.save('prob_qlora.npy', proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33175c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:59.424960Z",
     "iopub.status.busy": "2025-05-10T04:28:59.424190Z",
     "iopub.status.idle": "2025-05-10T04:28:59.427767Z",
     "shell.execute_reply": "2025-05-10T04:28:59.426988Z"
    },
    "papermill": {
     "duration": 0.015077,
     "end_time": "2025-05-10T04:28:59.429385",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.414308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python predict_lora.py\n",
    "#display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78274a98",
   "metadata": {
    "papermill": {
     "duration": 0.008455,
     "end_time": "2025-05-10T04:28:59.446358",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.437903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c349f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:59.465160Z",
     "iopub.status.busy": "2025-05-10T04:28:59.464557Z",
     "iopub.status.idle": "2025-05-10T04:28:59.469545Z",
     "shell.execute_reply": "2025-05-10T04:28:59.468812Z"
    },
    "papermill": {
     "duration": 0.016073,
     "end_time": "2025-05-10T04:28:59.471263",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.455190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%writefile make_submission.py\\nimport numpy as np\\nimport pandas as pd\\n\\ndf = pd.read_parquet(\"test.parquet\")\\npreds = np.average(\\n    [\\n        np.load(\"prob_m0.npy\"),\\n        np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\\n    ],\\n    axis=0,\\n    weights=[2, 1],\\n)\\nsub = pd.DataFrame({\\n    \"id\": df[\"id\"],\\n    \"winner_model_a\": preds[:, 0],\\n    \"winner_model_b\": preds[:, 1],\\n    \"winner_tie\": preds[:, 2],\\n})\\nsub.to_csv(\"submission.csv\", index=False)\\nprint(sub.head())\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%writefile make_submission.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "preds = np.average(\n",
    "    [\n",
    "        np.load(\"prob_m0.npy\"),\n",
    "        np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\n",
    "    ],\n",
    "    axis=0,\n",
    "    weights=[2, 1],\n",
    ")\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": df[\"id\"],\n",
    "    \"winner_model_a\": preds[:, 0],\n",
    "    \"winner_model_b\": preds[:, 1],\n",
    "    \"winner_tie\": preds[:, 2],\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b480d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:59.489825Z",
     "iopub.status.busy": "2025-05-10T04:28:59.489557Z",
     "iopub.status.idle": "2025-05-10T04:28:59.566476Z",
     "shell.execute_reply": "2025-05-10T04:28:59.565554Z"
    },
    "papermill": {
     "duration": 0.088126,
     "end_time": "2025-05-10T04:28:59.568252",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.480126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.002274        0.981056    0.016670\n",
      "1   211333        0.518577        0.127109    0.354313\n",
      "2  1233961        0.084269        0.766465    0.149266\n"
     ]
    }
   ],
   "source": [
    "#%%writefile make_submission.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "\n",
    "prob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\n",
    "prob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\n",
    "prob_qlora = np.load(\"prob_qlora.npy\")  # QLoRA\n",
    "\n",
    "# Combine predictions with weights\n",
    "# Adjust weights as needed for optimal performance\n",
    "preds = np.average(\n",
    "    [\n",
    "        prob_m0,       # Gemma2 results\n",
    "        prob_m3,       # Llama3 results\n",
    "        prob_qlora     # QLoRA results\n",
    "    ],\n",
    "    axis=0,\n",
    "    weights=[2, 0.99, 0]  # Weights for each model\n",
    ")\n",
    "\n",
    "# Create submission DataFrame\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": df[\"id\"],\n",
    "    \"winner_model_a\": preds[:, 0],\n",
    "    \"winner_model_b\": preds[:, 1],\n",
    "    \"winner_tie\": preds[:, 2],\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "011e59ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:28:59.589400Z",
     "iopub.status.busy": "2025-05-10T04:28:59.588707Z",
     "iopub.status.idle": "2025-05-10T04:28:59.592332Z",
     "shell.execute_reply": "2025-05-10T04:28:59.591511Z"
    },
    "papermill": {
     "duration": 0.014883,
     "end_time": "2025-05-10T04:28:59.593860",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.578977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python make_submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c17fe",
   "metadata": {
    "papermill": {
     "duration": 0.008926,
     "end_time": "2025-05-10T04:28:59.611612",
     "exception": false,
     "start_time": "2025-05-10T04:28:59.602686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5493674,
     "sourceId": 9102725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496762,
     "sourceId": 9107824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496920,
     "sourceId": 9108069,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 680.353178,
   "end_time": "2025-05-10T04:29:02.594258",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T04:17:42.241080",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07311ef62a7845a18b4ca01b09200186": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a926b0af09d4f0789bb7c350c10ed39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bcbf20c375d472cb20fe4f1eae8557b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c7fdf51373f4465918caec008bcc8ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7bc60c2f17664315b6ba59179430750d",
       "placeholder": "​",
       "style": "IPY_MODEL_66248c9bcbe54a509e6b863d99397714",
       "value": " 2/2 [01:18&lt;00:00, 34.83s/it]"
      }
     },
     "34b741274b374a90b8327015f541f48b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38915df660d7410eb8c91f71d3f8bb74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "48ca85f0a31f4ec9a18a74739fb8126a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "553b9bab795e4a198a62ac746c3555be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34b741274b374a90b8327015f541f48b",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_38915df660d7410eb8c91f71d3f8bb74",
       "value": 2.0
      }
     },
     "66248c9bcbe54a509e6b863d99397714": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6a8e7bf636d242c4bcda207bf70a3a98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "797e3899c06843c380bde69a23b714c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1a926b0af09d4f0789bb7c350c10ed39",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_48ca85f0a31f4ec9a18a74739fb8126a",
       "value": 2.0
      }
     },
     "7bc60c2f17664315b6ba59179430750d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80cc9dfd7c034fdba7eefc8378a19137": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "935177f0bbea4fc19ccf061fd9128298": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "938e957a5552408198d16b66199ea9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80cc9dfd7c034fdba7eefc8378a19137",
       "placeholder": "​",
       "style": "IPY_MODEL_935177f0bbea4fc19ccf061fd9128298",
       "value": " 2/2 [00:03&lt;00:00,  1.59s/it]"
      }
     },
     "9ae8e02b3fc54befadfc7165e7a685bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bc857bc4f75c40dd86e8bc8a6a7be99b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bfa5570dc3bc4382b908cfb61756fc62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cdd903f4d8614d94bfcd791f5f251073": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f44b0f8d3b914a77ad1addda20a9d3a1",
        "IPY_MODEL_797e3899c06843c380bde69a23b714c7",
        "IPY_MODEL_1c7fdf51373f4465918caec008bcc8ee"
       ],
       "layout": "IPY_MODEL_07311ef62a7845a18b4ca01b09200186"
      }
     },
     "d0d2e975197e4a78b02f36c81cdbe587": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e0f2ca5a0f4c47ecb89fd6e529e0a641",
        "IPY_MODEL_553b9bab795e4a198a62ac746c3555be",
        "IPY_MODEL_938e957a5552408198d16b66199ea9a0"
       ],
       "layout": "IPY_MODEL_6a8e7bf636d242c4bcda207bf70a3a98"
      }
     },
     "e0f2ca5a0f4c47ecb89fd6e529e0a641": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1bcbf20c375d472cb20fe4f1eae8557b",
       "placeholder": "​",
       "style": "IPY_MODEL_bc857bc4f75c40dd86e8bc8a6a7be99b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f44b0f8d3b914a77ad1addda20a9d3a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bfa5570dc3bc4382b908cfb61756fc62",
       "placeholder": "​",
       "style": "IPY_MODEL_9ae8e02b3fc54befadfc7165e7a685bd",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
